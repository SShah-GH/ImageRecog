{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines Global Variables data entries\n",
    "TOTAL_DATA = 150\n",
    "\n",
    "# Splits data into training(80%) and testing(20%)\n",
    "TRAINING_DATA = int(TOTAL_DATA*0.8) \n",
    "TESTING_DATA = int(TOTAL_DATA*0.2)\n",
    "\n",
    "# Number of input features\n",
    "FEATURE_NUMBER = 4\n",
    "\n",
    "# Number of classes and respective indexing\n",
    "CLASS_NUMBER = 3\n",
    "CLASSNAMES = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n",
       "11    12            4.8           3.4            1.6           0.2   \n",
       "81    82            5.5           2.4            3.7           1.0   \n",
       "139  140            6.9           3.1            5.4           2.1   \n",
       "10    11            5.4           3.7            1.5           0.2   \n",
       "44    45            5.1           3.8            1.9           0.4   \n",
       "\n",
       "             Species  \n",
       "11       Iris-setosa  \n",
       "81   Iris-versicolor  \n",
       "139   Iris-virginica  \n",
       "10       Iris-setosa  \n",
       "44       Iris-setosa  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>SepalLengthCm</th>\n      <th>SepalWidthCm</th>\n      <th>PetalLengthCm</th>\n      <th>PetalWidthCm</th>\n      <th>Species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>11</th>\n      <td>12</td>\n      <td>4.8</td>\n      <td>3.4</td>\n      <td>1.6</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>82</td>\n      <td>5.5</td>\n      <td>2.4</td>\n      <td>3.7</td>\n      <td>1.0</td>\n      <td>Iris-versicolor</td>\n    </tr>\n    <tr>\n      <th>139</th>\n      <td>140</td>\n      <td>6.9</td>\n      <td>3.1</td>\n      <td>5.4</td>\n      <td>2.1</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>5.4</td>\n      <td>3.7</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>45</td>\n      <td>5.1</td>\n      <td>3.8</td>\n      <td>1.9</td>\n      <td>0.4</td>\n      <td>Iris-setosa</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 471
    }
   ],
   "source": [
    "# Read in data\n",
    "data = pd.read_csv('../DataSets/Iris.csv')\n",
    "\n",
    "#Splits into 2 classes instead of 3\n",
    "#data = data[0:100]\n",
    "\n",
    "# Shuffle data\n",
    "data = data.sample(frac = 1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling feature data and splicing it for training/testing using 80-20 ratio\n",
    "x_train = data.iloc[:TRAINING_DATA, 1:FEATURE_NUMBER+1].values\n",
    "x_test = data.iloc[TRAINING_DATA:, 1:FEATURE_NUMBER+1].values\n",
    "\n",
    "# Standardizes the data by dividing the entries by standard deviation (i.e calculating how many standard deviations the entries are from the center)\n",
    "x_train = (x_train - np.mean(x_train, axis=0)) / np.std(x_train, axis=0)\n",
    "x_test = (x_test - np.mean(x_test, axis=0)) / np.std(x_test, axis=0)\n",
    "\n",
    "# Reshape for matrix multiplication\n",
    "x_train = x_train.reshape(FEATURE_NUMBER, TRAINING_DATA)\n",
    "x_test = x_test.reshape(FEATURE_NUMBER, TESTING_DATA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling label data and splicing it for training/testing using 80-20 ratio\n",
    "y_train_data = data.iloc[:TRAINING_DATA, -1].values\n",
    "y_test_data = data.iloc[TRAINING_DATA:, -1].values\n",
    "\n",
    "# Create vectorized representations of each data point's class membership\n",
    "y_train = np.zeros(shape=(CLASS_NUMBER, TRAINING_DATA))\n",
    "y_test = np.zeros(shape=(CLASS_NUMBER, TESTING_DATA))\n",
    "\n",
    "# Setting numeric labels for each data point's class\n",
    "for row in range(CLASS_NUMBER):\n",
    "    y_train[row, :TRAINING_DATA]  = [ele == CLASSNAMES[row] for ele in y_train_data]\n",
    "    y_test[row, :TESTING_DATA] = [ele == CLASSNAMES[row] for ele in y_test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize weights and bias matricies\n",
    "w = np.random.rand(CLASS_NUMBER, FEATURE_NUMBER)\n",
    "b = np.random.rand(CLASS_NUMBER, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "30\n[[ 6.23175268e-02 -1.36227672e-01  1.41369588e+00  1.08103756e-01\n   2.11248741e-01  2.05196373e+00  1.04263828e+00 -1.05908405e+00\n   7.67466552e-01  1.71386914e+00 -3.36762651e-01  4.44366596e-01\n   1.36502976e+00 -1.13523549e+00  4.33007535e-01  2.89211333e+00\n  -2.22088994e+00 -8.23177191e-01 -1.09599182e+00 -1.17398488e+00\n  -8.43281862e-01  1.43907604e-01 -2.86098227e-01  1.56768835e-01\n   1.75084373e+00  2.12415850e-01  1.28189441e+00  1.24077762e+00\n  -6.24500970e-01  9.77709303e-01]\n [-7.29282006e-01  1.10637223e+00  1.54033498e-01 -1.06606313e+00\n   1.40381212e+00  9.55686963e-01  2.13260047e+00  3.76064409e-01\n   4.31099109e-01 -1.72722833e-01  5.07542883e-01  9.77550336e-01\n   3.02721419e+00  5.81223021e-01  1.41930652e+00  3.30177959e+00\n  -1.23876588e+00 -2.66424182e-01 -9.49611541e-01 -8.83467384e-01\n   1.15244175e+00  8.86381085e-02  1.27258597e+00  1.88132295e+00\n   1.32720839e+00 -2.87496185e-01  1.52369749e+00  1.82166729e+00\n  -2.63534444e-01  1.82570998e-01]\n [ 3.26171545e-03  2.52025899e-01  5.47611140e-01 -7.62318891e-01\n  -4.19775625e-02  1.22830280e+00  9.22006518e-01 -1.07902967e+00\n   1.85483345e-01  6.31380796e-01 -7.14571654e-01  4.38817266e-01\n   5.57386272e-01 -1.90291929e+00  6.48915212e-01  3.67438544e+00\n  -2.35628496e+00 -1.09595649e+00 -8.33014101e-01 -1.63085025e+00\n   3.70827891e-03  5.13697609e-01  5.28417042e-01  4.77452277e-01\n   1.99554932e+00  6.28010199e-01  1.35284655e+00  7.23306106e-01\n  -2.55146949e-01  7.24737018e-01]]\n"
     ]
    }
   ],
   "source": [
    "#Class coresspondance function, returns a vector (z), which will be input into softmax function\n",
    "#The highest number in z represents the correct class\n",
    "def class_corr(weights, bias, x):\n",
    "    \n",
    "    numData = x.shape[1]\n",
    "    print(numData)\n",
    "    z = weights.dot(x)\n",
    "    #print(z)\n",
    "    for input in range(numData):\n",
    "        z[0:CLASS_NUMBER, input:input+1] += bias\n",
    "\n",
    "    return z\n",
    "\n",
    "z = class_corr(w, b, x_test)\n",
    "print(z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.41740171 0.16837815 0.58673571 0.57875365 0.19716826 0.56403648\n  0.20573922 0.16179867 0.43991824 0.67098824 0.24926945 0.27035679\n  0.1488822  0.14227033 0.20315787 0.21309469 0.220095   0.28520433\n  0.28914821 0.33665671 0.09354414 0.29466705 0.1248339  0.1251864\n  0.34107485 0.32032236 0.29876849 0.29553851 0.25763135 0.44883139]\n [0.18913283 0.58336451 0.16648615 0.17887908 0.64977156 0.18845178\n  0.6119017  0.67959786 0.31426025 0.10171346 0.57989089 0.46078256\n  0.78473009 0.7917039  0.54472569 0.32098773 0.58768044 0.49768079\n  0.33472847 0.45015017 0.68825336 0.27882284 0.59328106 0.70229808\n  0.22328893 0.19430243 0.38049463 0.52831295 0.36962768 0.2026558 ]\n [0.39346546 0.24825735 0.24677815 0.24236728 0.15306018 0.24751174\n  0.18235908 0.15860347 0.24582151 0.2272983  0.17083966 0.26886065\n  0.06638771 0.06602577 0.25211644 0.46591758 0.19222456 0.21711487\n  0.37612333 0.21319311 0.2182025  0.42651011 0.28188504 0.17251553\n  0.43563622 0.48537521 0.32073689 0.17614854 0.37274097 0.34851281]]\n"
     ]
    }
   ],
   "source": [
    "#Softmax Function\n",
    "#expnentiates all elements of the z vector and divides by their sum to see class probability\n",
    "def softmax(z):\n",
    "\n",
    "    #creates empty probability array\n",
    "    y_hat = np.empty([z.shape[0], z.shape[1]])\n",
    "    numData = z.shape[1]\n",
    "\n",
    "    #exponentiates the matrix\n",
    "    z_exp = np.exp(z)\n",
    "\n",
    "    #Converts z vector into probability distribution\n",
    "    for input in range(numData):\n",
    "        z_sum = np.sum(z_exp[0:CLASS_NUMBER, input:input+1])\n",
    "        y_hat[0:CLASS_NUMBER, input:input+1] = z_exp[0:CLASS_NUMBER, input:input+1]/z_sum\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "y_pred = softmax(z)\n",
    "print(softmax(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.3748808447088445"
      ]
     },
     "metadata": {},
     "execution_count": 477
    }
   ],
   "source": [
    "#Loss function\n",
    "#Returns the sum of all probabilities compared to the actual class\n",
    "def cost(y, y_pred):\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    #Finds number of data samples\n",
    "    numData = y.shape[1]\n",
    "\n",
    "    #Manipulates y_pred into right form for the loss function\n",
    "    y_pred = -1*np.log(y_pred)\n",
    "\n",
    "    #iterates over all data samples\n",
    "    for column in range(numData):\n",
    "\n",
    "        #Computes total class loss for each data sample and sums them\n",
    "        y_cur = y[0:CLASS_NUMBER, column:column+1].transpose()\n",
    "        y_pred_cur = (y_pred[0:CLASS_NUMBER, column:column+1])\n",
    "        total_loss += y_cur.dot(y_pred_cur)[0][0]\n",
    "\n",
    "    #Divdes by number of data samples to find average error\n",
    "    total_loss = total_loss/numData\n",
    "    return total_loss\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "cost(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the gradients of the loss with respect to weights and bias\n",
    "def findGradients(x, y, y_pred):\n",
    "    # Number of data samples\n",
    "    numData = x.shape[0]\n",
    "\n",
    "    # Calculate error for gradient computations\n",
    "    error = y_pred - y\n",
    "    error = error.transpose()\n",
    "\n",
    "    # Dictionary for holding gradient values\n",
    "    gradientDict = dict()\n",
    "    gradientDict['gradWeights'] = np.array((1/numData) * x.dot(error))\n",
    "    gradientDict['gradBias'] =  (1/numData) * np.sum(error)\n",
    "\n",
    "    return gradientDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}