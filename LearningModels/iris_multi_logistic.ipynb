{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines Global Variables data entries\n",
    "TOTAL_DATA = 150\n",
    "\n",
    "# Splits data into training(80%) and testing(20%)\n",
    "TRAINING_DATA = int(TOTAL_DATA*0.8) \n",
    "TESTING_DATA = int(TOTAL_DATA*0.2)\n",
    "\n",
    "# Number of input features\n",
    "FEATURE_NUMBER = 4\n",
    "\n",
    "# Number of classes and respective indexing\n",
    "CLASS_NUMBER = 3\n",
    "CLASSNAMES = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n",
       "103  104            6.3           2.9            5.6           1.8   \n",
       "49    50            5.0           3.3            1.4           0.2   \n",
       "0      1            5.1           3.5            1.4           0.2   \n",
       "27    28            5.2           3.5            1.5           0.2   \n",
       "104  105            6.5           3.0            5.8           2.2   \n",
       "\n",
       "            Species  \n",
       "103  Iris-virginica  \n",
       "49      Iris-setosa  \n",
       "0       Iris-setosa  \n",
       "27      Iris-setosa  \n",
       "104  Iris-virginica  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>SepalLengthCm</th>\n      <th>SepalWidthCm</th>\n      <th>PetalLengthCm</th>\n      <th>PetalWidthCm</th>\n      <th>Species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>103</th>\n      <td>104</td>\n      <td>6.3</td>\n      <td>2.9</td>\n      <td>5.6</td>\n      <td>1.8</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>50</td>\n      <td>5.0</td>\n      <td>3.3</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>28</td>\n      <td>5.2</td>\n      <td>3.5</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>104</th>\n      <td>105</td>\n      <td>6.5</td>\n      <td>3.0</td>\n      <td>5.8</td>\n      <td>2.2</td>\n      <td>Iris-virginica</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 759
    }
   ],
   "source": [
    "# Read in data\n",
    "data = pd.read_csv('../DataSets/Iris.csv')\n",
    "\n",
    "#Splits into 2 classes instead of 3\n",
    "#data = data[0:100]\n",
    "\n",
    "# Shuffle data\n",
    "data = data.sample(frac = 1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling feature data and splicing it for training/testing using 80-20 ratio\n",
    "x_train = data.iloc[:TRAINING_DATA, 1:FEATURE_NUMBER+1].values\n",
    "x_test = data.iloc[TRAINING_DATA:, 1:FEATURE_NUMBER+1].values\n",
    "\n",
    "# Standardizes the data by dividing the entries by standard deviation (i.e calculating how many standard deviations the entries are from the center)\n",
    "x_train = (x_train - np.mean(x_train, axis=0)) / np.std(x_train, axis=0)\n",
    "x_test = (x_test - np.mean(x_test, axis=0)) / np.std(x_test, axis=0)\n",
    "\n",
    "# Reshape for matrix multiplication\n",
    "x_train = x_train.reshape(FEATURE_NUMBER, TRAINING_DATA)\n",
    "x_test = x_test.reshape(FEATURE_NUMBER, TESTING_DATA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling label data and splicing it for training/testing using 80-20 ratio\n",
    "y_train_data = data.iloc[:TRAINING_DATA, -1].values\n",
    "y_test_data = data.iloc[TRAINING_DATA:, -1].values\n",
    "\n",
    "# Create vectorized representations of each data point's class membership\n",
    "y_train = np.zeros(shape=(CLASS_NUMBER, TRAINING_DATA))\n",
    "y_test = np.zeros(shape=(CLASS_NUMBER, TESTING_DATA))\n",
    "\n",
    "# Setting numeric labels for each data point's class\n",
    "for row in range(CLASS_NUMBER):\n",
    "    y_train[row, :TRAINING_DATA]  = [ele == CLASSNAMES[row] for ele in y_train_data]\n",
    "    y_test[row, :TESTING_DATA] = [ele == CLASSNAMES[row] for ele in y_test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize weights and bias matricies\n",
    "w = np.random.rand(CLASS_NUMBER, FEATURE_NUMBER)\n",
    "b = np.random.rand(CLASS_NUMBER, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3, 30)"
      ]
     },
     "metadata": {},
     "execution_count": 763
    }
   ],
   "source": [
    "#Class coresspondance function, returns a vector (z), which will be input into softmax function\n",
    "#The highest number in z represents the correct class\n",
    "def class_corr(weights, bias, x):\n",
    "    \n",
    "    numData = x.shape[0]\n",
    "    z = weights.dot(x)\n",
    "    #print(z)\n",
    "    for input in range(numData):\n",
    "        z[0:CLASS_NUMBER, input:input+1] += bias\n",
    "\n",
    "    return z\n",
    "\n",
    "z = class_corr(w, b, x_test)\n",
    "z.shape \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 0.63405516,  0.38296313,  0.65433425,  0.42328965, -0.07715597,\n",
       "        -0.68608035,  1.88128163,  1.86577032,  1.36604187,  0.64635351,\n",
       "        -0.45811521, -0.8786122 ,  0.09259061,  0.02678315, -1.27516967,\n",
       "         0.72569446, -1.52649351, -1.26641019,  1.41026293,  0.97837831,\n",
       "         1.6236292 ,  1.64723908, -1.04175767, -0.21575352, -2.24648079,\n",
       "        -1.71962742, -0.95280246, -1.22182041,  0.44922389,  0.68814984],\n",
       "       [ 0.18344575,  0.14651644,  0.19657114,  2.10220616, -1.5624777 ,\n",
       "        -2.45974064,  0.25395219, -0.71632768,  0.35508429, -0.6608705 ,\n",
       "         0.69462987, -1.21777404,  0.64202334,  0.10990847, -1.30781058,\n",
       "         2.25419099, -0.98633315, -0.33394466,  2.23540173,  1.69558172,\n",
       "         2.7478101 ,  2.27760702,  0.31048823, -0.97789591, -1.64039441,\n",
       "         0.02601551, -1.65476768, -2.04824364,  0.37319668,  0.28371399],\n",
       "       [ 0.18249909,  0.47052043,  0.14909461,  1.68734904, -1.79427307,\n",
       "        -2.60891557, -0.3499128 , -1.46630281, -0.07000144, -0.95566881,\n",
       "         1.12256885, -0.84115981,  1.03579333,  0.21901359, -0.90969407,\n",
       "         2.28399138, -0.94844424,  0.40215004,  1.40456823,  1.06591227,\n",
       "         2.26102285,  1.96262764,  0.74704503, -0.96230048, -1.55166957,\n",
       "         0.01440847, -1.46250265, -1.71856828,  0.48011581,  0.18730905]])"
      ]
     },
     "metadata": {},
     "execution_count": 764
    }
   ],
   "source": [
    "#Softmax Function\n",
    "#expnentiates all elements of the z vector and divides by their sum to see class probability\n",
    "def softmax(z):\n",
    "\n",
    "    #creates empty probability array\n",
    "    y_hat = np.empty([z.shape[0], z.shape[1]])\n",
    "    numData = z.shape[0]\n",
    "\n",
    "    #exponentiates the matrix\n",
    "    z_exp = np.exp(z)\n",
    "\n",
    "    #Converts z vector into probability distribution\n",
    "    for input in range(numData):\n",
    "        z_sum = np.sum(z_exp[0:CLASS_NUMBER, input:input+1])\n",
    "        y_hat[0:CLASS_NUMBER, input:input+1] = z_exp[0:CLASS_NUMBER, input:input+1]/z_sum\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "softmax(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "cost() missing 1 required positional argument: 'y_pred'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-765-7de226fcdcea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mcost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: cost() missing 1 required positional argument: 'y_pred'"
     ]
    }
   ],
   "source": [
    "#Loss function\n",
    "#Returns the sum of all probabilities compared to the actual class\n",
    "def cost(x, y, y_pred):\n",
    "    # Number of data samples\n",
    "    numData = x.shape[0]\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(numData):\n",
    "        z = class_corr(w, b, x[:, i])\n",
    "        y_pred = softmax(z)\n",
    "        y_actual = np.reshape(y[:, i], [1, 3])\n",
    "        total_loss += -1*(y_actual.dot(np.log(y_pred.transpose())))\n",
    "\n",
    "    total_loss = total_loss/numData\n",
    "    return total_loss\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "cost(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the gradients of the loss with respect to weights and bias\n",
    "def findGradients(x, y, y_pred):\n",
    "    # Number of data samples\n",
    "    numData = x.shape[0]\n",
    "\n",
    "    # Calculate error for gradient computations\n",
    "    error = y_pred - y\n",
    "    error = error.transpose()\n",
    "\n",
    "    # Dictionary for holding gradient values\n",
    "    gradientDict = dict()\n",
    "    gradientDict['gradWeights'] = np.array((1/numData) * x.dot(error))\n",
    "    gradientDict['gradBias'] =  (1/numData) * np.sum(error)\n",
    "\n",
    "    return gradientDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}